{"name":"eatiht","tagline":"A simple tool used to extract an article's text in html documents.","body":"eatiht - written by rodrigo palacios\r\n======\r\n\r\n---\r\n\r\nemail me: rodrigopala91@gmail.com\r\n\r\nfollow me [@rodricios](https://twitter.com/rodricios) and [share eatiht!](http://hrefshare.com/3276)\r\n\r\n---\r\n\r\n\r\nSummary\r\n-------\r\n\r\nClick here to view [eatiht's source.](https://github.com/rodricios/eatiht/commit/a872089a8df81d9fed6cd33312d695f19aa111ae#diff-6556ac4c9a425d802573fa15e0141773R1)\r\n\r\n---\r\n\r\neatiht is one possible solution in a line of many imperfect solutions\r\nto one or more problems in some subfield of the field called\r\n\"[data extraction](http://en.wikipedia.org/wiki/Data_extraction).\"\r\n\r\nPut plainly, eatiht tries to extract the central text from a\r\ngiven website.\r\n\r\nShout out to the folks at [/r/Python](http://www.reddit.com/r/Python/comments/2pqx2d/just_made_what_i_consider_my_first_algorithm_and/),[/r/compsci](http://www.reddit.com/r/compsci/comments/2ppyot/just_made_what_i_consider_my_first_algorithm_it/),[/r/programming](http://www.reddit.com/r/programming/comments/2pq32f/just_made_what_i_consider_my_first_algorithm_it/). This project started [trending](https://github.com/trending?l=python) largely due to the positive response I got from those guys, and for that I'm grateful. Now onto the main article of this page ;)\r\n\r\nBackground\r\n----------\r\n\r\n(skip if you've read eatiht's readme)\r\n\r\nAfter searching through the deepest crevices of the internet for some tool|library|module that could effectively extract the main content from a website (ignoring text from ads, sidebar links, etc.), I was slightly disheartened by the apparent ambiguity caused by this content-extraction problem.\r\n\r\nMy survey resulted in some of the following solutions:\r\n\r\n* [boilerpipe](https://code.google.com/p/boilerpipe/) - *Boilerplate Removal and Fulltext Extraction from HTML pages*. Java library written by Christian Kohlschütter\r\n* [\"The Easy Way to Extract Useful Text from Arbitrary HTML\"](http://ai-depot.com/articles/the-easy-way-to-extract-useful-text-from-arbitrary-html/) - a Python tutorial on implementing a neural network for html content extraction. Written by alexjc\r\n* [Pyteaser's Cleaners module](https://github.com/xiaoxu193/PyTeaser/blob/master/goose/cleaners.py) - from what I can tell, it's a purely heuristic-based process\r\n* [\"Text Extraction from the Web via Text-to-Tag Ratio\"](http://www.cse.nd.edu/~tweninge/pubs/WH_TIR08.pdf) - a thesis on Text-to-Tag-heuristic driven clustering as a solution for the problem at hand. Written by Tim Weninger & William H. Hsu\r\n\r\nThe number of research papers I found on the subject largely outweighs the number available open-source projects. This is my attempt at balancing out the disparity.\r\n\r\n\r\nThe original algorithm*\r\n=======================\r\n\r\nThere's two assumptions:\r\n\r\n1. \"Valuable\" text is \"lengthy\"\r\n2. \"Valuable\" text come in \"*packs*\"\r\n\r\nBy \"valuable,\" and without getting too philosophical,\r\nI mean that it's the text you're meant to read.\r\n\r\nBy \"lengthy,\" I mean there is a specific value that is used\r\nto filter out text for a specific computation. More on this\r\nlater.\r\n\r\nBy \"packs,\" I can describe it in two ways:\r\n\r\nVisually\r\n--------\r\n\r\nHead over to this [page](http://en.wikipedia.org/wiki/Orangutan#FreeTheOrangutans); don't forget to come back!\r\n\r\nDo you see how the article is centered, how the edges are sharp, how the background-color tones gently compliment\r\neach other, yet they serve to contrast differing regions of importance?\r\n\r\nDid you also notice how there were groups of text; clearly aligned both vertically and horizontally?\r\n\r\nImagine emboldening every letter on that page to the point where each letter just looks like a somewhat\r\nrectangular blob and they're starting to overlap with their nearest neighbors. A *pack*\r\ncan be described as the largest blob(s). As a side note, this illustration is also a useful description\r\nof a method used for visual text-extraction.\r\n\r\nSlightly-technical\r\n------------------\r\n\r\nWarning: this part is better understood if you have some understanding\r\nof HTML and XPath. Scroll all the way down or click this [link](#crash-course-on-html-and-xpath) for a quick crash course.\r\n\r\n---\r\n\r\nNow consider a typical HTML document that eatiht will likely do well on:\r\n\r\n    foobar.html:\r\n    <html>\r\n        <head></head>\r\n        <body>\r\n            <div>\r\n                <article>\r\n                    <p>This is a story about the life of Foo</p>\r\n                    <p>The life of Foo was one of great foo</p>\r\n                    <p>Foo foo, foo foo foo. Foo, foofoo?</p>\r\n                    <p>Foo was no stranger to foo. For Foo did foo</p>\r\n                </article>\r\n            </div>\r\n            <div>\r\n                <div>\r\n                    <p>Buy Bar Now!</p>\r\n                    <p>Get The Bar Next Door!\r\n                    <p>Increase Your Bar!</p>\r\n                    <p>Never Bar again!</p>\r\n                </div>\r\n            <div>\r\n                <footer>\r\n                    <p>Who the hell is Boo. Who the hell is Far?</p\r\n                </footer>\r\n            </div>\r\n        </body>\r\n    </html>\r\n\r\nThe selection\r\n-------------\r\n\r\nThe [original implementation of eatiht](https://github.com/rodricios/eatiht/commit/a872089a8df81d9fed6cd33312d695f19aa111ae#diff-6556ac4c9a425d802573fa15e0141773R1) had the following xpath:\r\n\r\n    //body//*[not(self::script or self::style or self::i or self::b or self::strong or self::span or self::a)]/text()[string-length(normalize-space()) > 20]/..\r\n\r\nBut that expression can, in essence, be simplified down to:\r\n\r\n    //body//*/text()[string-length(normalize-space()) > 20]/..\r\n\r\nWhat was taken out was the *but the following* in *select all but the following.*\r\n\r\nNow for the xpath expression after the '*', but before the '/..':\r\n\r\n    /text()[string-length(normalize-space()) > 20]\r\n\r\nThis is basically saying *select all text nodes that have a _normalized_ string length greater than 20.*\r\n\r\nNote about \"text nodes.\" [The text node is an html element](http://www.w3schools.com/dom/dom_nodetype.asp) (scroll to TEXT_TYPES) implicitly declared with the inclusion of text (this includes white spaces and new lines - like between div and p tags).\r\n\r\n    ...\r\n    <div>\r\n        <p>foo</p>\r\n    </div>\r\n    ...\r\n\r\nIf we were to extract the text of those three nodes (using an expression like: \"//text()\"), here's what we'd get:\r\n\r\n    \"\\n         foo\\n        \\n\"          #or something similar\r\n\r\n\r\nThe [\"normalized-space()\"](http://www.w3schools.com/xpath/xpath_functions.asp) of a text node means that said text node will have leading and trailing white spaces removed, and internal white spaces will also be reduced down to\r\na single white space, if multiple white spaces occur next to each other.\r\n\r\n    \"\\nfoo\\n\\n\"\r\n\r\nFinally, the last sub-expression is:\r\n\r\n    /..\r\n\r\n*Select the __parent__ of the last node selected* or *parent of*. Let that sink in for a minute.\r\n\r\nA common misconception after [executing the entire xpath](https://github.com/rodricios/eatiht/commit/a872089a8df81d9fed6cd33312d695f19aa111ae#diff-6556ac4c9a425d802573fa15e0141773R72) expression is that the expression\r\nwill result in the following nodes:\r\n\r\n    /html/body/div/article\r\n    /html/body/div/div\r\n    /html/body/div/footer\r\n\r\nwhen in fact we get:\r\n\r\n    /html/body/div/article/p          <-- Set A\r\n    /html/body/div/div/p              <-- Set B\r\n    /html/body/div/footer/p           <-- Set C\r\n\r\nWhy is that an important clarification/distinction?\r\n\r\nThe partitioning\r\n----------------\r\n\r\nIf you didn't notice, I assigned names to the resulting sets of nodes.\r\n\r\nLet's count the number of elements in each set:\r\n\r\nSet A has 4 elements; each text node contained a normalized string with length greater than 20.\r\n\r\nSet B has only 1 element; all but one text node fell short to our string length requirement.\r\n\r\nSet C has 1 element, and it's the only element with that parent. Or in other words, its\r\nparent has only one child, and that child is in Set C.\r\n\r\nNow you may think, \"Alright, we can create a frequency distribution across each Set, and we're good!\"\r\n\r\nJust to see what that means, here's the freq. distribution (histogram)\r\n\r\n    Set A | — — — —\r\n    Set B | —\r\n    Set C | —\r\n\r\nWe technically have found collection of nodes with our desired output. But eatiht takes it **two steps further**.\r\n\r\nActually, the real reason why I didn't stop here is, I don't know. This solution came after many\r\nsleepless nights, a lot of coffee in addition to some other things that decreased sleep, improved focus,\r\nand increased short to mid-term memory loss.\r\n\r\nBack to the algorithm!\r\n\r\n###The first step further\r\nWhat eatiht does before creating the histogram is it [*partitions*](https://github.com/rodricios/eatiht/commit/a872089a8df81d9fed6cd33312d695f19aa111ae#diff-6556ac4c9a425d802573fa15e0141773R77) each Set. How? With a [REGEX pattern](https://github.com/rodricios/eatiht/commit/a872089a8df81d9fed6cd33312d695f19aa111ae#diff-6556ac4c9a425d802573fa15e0141773R18) designed to split a string per sentence (it tries to, at least).\r\n\r\nWhat does our histogram look like now?\r\n\r\n    Set A | — — — — — —\r\n    Set B | —\r\n    Set C | — —\r\n\r\nLet's replace the Set names A, B, and C with their xpath equivalents:\r\n\r\n    /html/body/div/article/p    | — — — — — —\r\n    /html/body/div/div/p        | —\r\n    /html/body/div/footer/p     | — —\r\n\r\n###The second step further\r\n\r\nThis one's quick:\r\n\r\n    /html/body/div/article      | — — — — — —\r\n    /html/body/div/div          | —\r\n    /html/body/div/footer       | — —\r\n\r\n\r\nYup, all we do is drop the last path, effectively getting the **parent** of the **parent** of the **text nodes**.\r\n\r\nThe merging\r\n-----------\r\n\r\nWhen you find the *argmax* of some function or random variable, what you are in fact doing is finding the max value\r\nin said function, and getting the associated key that points to that value. In other words, you are requesting the\r\nargument that, when passed into a given function, will output the maximum value. And if getting told the same thing\r\nin two different ways wasn't enough, here's some pseudo-code:\r\n\r\n    argmax = keyof(max([ value for key,value in histogram ]))\r\n\r\nThe output (argmax) in our test case is the xpath:\r\n\r\n    /html/body/div/article\r\n\r\nHere's where we build the [histogram](https://github.com/rodricios/eatiht/commit/a872089a8df81d9fed6cd33312d695f19aa111ae#diff-6556ac4c9a425d802573fa15e0141773R46) and here's where we find the [argmax](https://github.com/rodricios/eatiht/commit/a872089a8df81d9fed6cd33312d695f19aa111ae#diff-6556ac4c9a425d802573fa15e0141773R85)\r\n\r\nNow that we've found the xpath, we can target the subtree in the original HTML tree. But instead of executing\r\nanother xpath query on the entire HTML tree, recall that we built a set of text nodes early on in **the\r\nselection**. The selection provided a set of text nodes that satisfied our first assumption (some length).\r\n\r\nWith a simple list comprehension, we can acquire the **some**, but not all of text nodes that exist in the desired\r\nsubtree - yes, this area can be improved, and it is one of many top priorities for me right now.\r\n\r\nThe [string-forming list-comprehenesion](https://github.com/rodricios/eatiht/commit/a872089a8df81d9fed6cd33312d695f19aa111ae#diff-6556ac4c9a425d802573fa15e0141773R87) is something like this:\r\n\r\n    article_text = ' '.join([ textnode.text for textnode in textnodes\r\n                                        if argmax in textnode.xpath ])\r\n\r\nConclusion\r\n----------\r\n\r\nThat's what the eatiht algorith is and does. Simple, no? Yes! It is! From a \"high-level\" perspective, this is\r\nhow I might write the pseudo-code of the algorithm in a technical paper:\r\n\r\n    xpath_to_parent_of_text_nodes = '/path/to/text()/..'\r\n    splitter = TextSplitter()\r\n    text_node_parents = HtmlTree.execute_xpath_expression(xpath_to_parent_of_text_nodes)\r\n\r\n    for node in text_node_parents:\r\n        partitions = []\r\n        for child in node.children:\r\n            partitions.insert( splitter.split( child ))\r\n        node.children = partitions\r\n\r\n    histogram = {}\r\n    for node in text_node_parents:\r\n        histogram[node.parent.path] = node.children.count\r\n\r\n    max_path = argmax( histogram )\r\n    main_text_nodes = [ node for node in text_node_parents if max_path in node.path ]\r\n\r\nFrom this high-level perspective, one can see the simplicity in this algorithm. Weighing in at just 13 l.o.p.c.,\r\nit's so simple; I would be surprised if this hasn't been thought of before. If it hasn't, cool! I might have something here.\r\nIt it has, cool! I for sure have something here. Both the former and the latter points lead to\r\nthe fact that this code is being tested and experimented with.\r\n\r\nThat fact alone tells me it should be maintained for stability's sake. It also tells me that there's\r\nwork to be done.\r\n\r\nSome considerations\r\n-------------------\r\n\r\nOne simple area where improvements can possibly be made is risk the extra cpu cycles to do a full\r\nsub-tree search in the HTML-tree rather than the list-comprehension filtering done in the\r\nmerging step, right before the conclusion section. This may or may not be rewarding.\r\n\r\nA concrete justification for the partitioning step eludes me. Although intuitively - as were most of the\r\nactions I took in this project - it appears that partitioning by sentences is a way to \"boost\"\r\nthe score or likelihood that a single paragraph with 21 sentences will be extracted than a subtree with\r\n20 single-sentence paragraphs, divs, etc. - as is sometimes the case with sidebar ads.\r\n\r\n##Complexity\r\n\r\nWhat is this algorithm's complexity? I'm not very skilled at algorithm analysis, but I'll try to talk\r\nmyself through this. Those who know their stuff can laugh at my likely-to-fail attempt, but if you do laugh,\r\nit wont be at my expense, it'll be at yours! Please help a brother out and send me your complexity analysis :)\r\n\r\nSo, as the xpath search is more or less out of my hands, I'm going to assume they implemented a binary search,\r\nthrough all vertices v, with an average complexity of:\r\n\r\n    eatiht's complexity += O(log(v))\r\n\r\nSay we found M text node parents (from **the selection** step, with each parent having an average of N children text nodes\r\n\r\nWith those assumptions, in **the partitioning** step we execute a\r\n\r\n    for parent in parents:\r\n        for child in parent.children:\r\n            ...\r\n\r\nThis would translate over to M parents over an average of N children:\r\n\r\n    eatiht's complexity += O(M*N) + O(log(v))\r\n\r\nIn building the frequency distribution, we iterate over the parents:\r\n\r\n    eatiht's complexity += O(M) + O(M*N) + O(log(v))\r\n\r\nNow in calculating the argmax of the histogram, the number of elements,\r\nat most is M, but it may also be less than M. The reason for this is because\r\nwe build the histogram not with the path of the parent of the text node (PoTN)\r\nbut the path of **parent** of the **parent** of the text node - it may be that\r\nsome PoTN's have the same parent, and thus the same path.\r\n\r\nBut let's just say the complexity is O(M) (would the actual complexity be O(M/2)?)\r\n\r\n    eatiht's complexity += O(M) + O(M) + O(M*N) + O(log(v))\r\n\r\nFinally, we have one last linear search for building up a list of final article text nodes:\r\n\r\n    eatiht's complexity += O(M) + O(M) + O(M) + O(M*N) + O(log(v))\r\n\r\nNow if you took a intro to comp. sci. theory class, you might recall that the final\r\ncomplexity is sometimes shortened to its component with the highest complexity.\r\n\r\nIn this case, I'm not sure which is the maximum. At first glance, I'd want to say\r\nthat O(M*N) is, but then I start thinking about all these corner cases. So I'll leave\r\nit to whomever wishes to find the solution in either my pseudo-analysis, or yours!\r\n\r\nThere were some more thought's I wanted to get in writing, but I've already spent the\r\nentire morning writing this up so I think I'll end it here for today. If you read\r\nall the way to here, I thank you for taking the time to do so. Send me your thoughts,\r\nsuggestions, criticisms, praise, venerations, admirations, palpitations, nude pics\r\n(just kidding, don't do at least 4 of those things in series; likely to cause\r\nunintended consequences!) to rodrigopala91@gmail.com or [@rodricios](https://twitter.com/rodricios).\r\n\r\nP.S. I have yet to decide whether or not I should test this algorithm against\r\na large dataset. Not because I don't want to, but because it would be an endeavor I\r\ndo not know how to approach in a more scientific manner, nor how to correctly\r\nallocate the time for (I would not want to tackle this head-on, commit and use a lot of\r\ntime, and then go and find out that the tests I would have done were not done correctly,\r\nand, in other words, were pointless).\r\n\r\nP.P.S There's some thoughts I had about the lookback-current-node-lookfront scope that's\r\npresent in this implementation, and how it reminds me of what is known as a \"kernel\"\r\nor \"filter\" matrix calculus and matrix transformations in the field of computer vision.\r\nIt also reminds of some of the nltk classifier implementations. But whatever it is I'm\r\ntrying to say right now will come out like a mumbly mess, so I'll leave it at that.\r\n\r\n\r\nCrash Course on HTML and XPath\r\n------------------------------\r\n\r\nHere's a quick crash course in case you're unsure if you *got what it takes.*\r\nOk, this really isn't on html, more like pseudo-html/trees.\r\n\r\nThis is a tree:\r\n\r\n    Graph A\r\n          root of tree:     •\r\n                           _|_\r\n                          |   |\r\n                leaves:   •   •\r\n\r\n\r\nLet's give those dots (aka: vertexes, nodes, elements, tags) *names*\r\n\r\n    Graph B\r\n                           A•\r\n                           _|_\r\n                          |   |\r\n                         b•   •c\r\n\r\nNote: The name 'A' is reserved for the root of the tree. There is no limit to the number of 'b' and 'c' nodes\r\none wishes to use, nor limit to how far down a tree can reach, as demonstrated here:\r\n\r\n    Graph C\r\n                           A•      <-from this level\r\n                          __|__\r\n                         |     |\r\n    from this level->   b•     •c  <-to this level\r\n                        _|_\r\n                       |   |\r\n      to this level-> c•   •c\r\n    ________________________________________________\r\n       is one level         +        is one level   = there is two levels\r\n\r\n\r\nThat's it for your review of html.\r\n\r\nAs for xpath, let's refer back to Graph C:\r\n\r\n    Graph C\r\n                           A•\r\n                          __|__\r\n                         |     |\r\n                        b•     •c\r\n                        _|_\r\n                       |   |\r\n                      c•   •c\r\n\r\n[XPath](http://www.w3schools.com/xpath/xpath_intro.asp) is, among other things,\r\na querying tool.\r\n\r\nTo query for (select) the root node A:\r\n\r\n    /A\r\n\r\nTo **absolutely** query for the only node named b:\r\n\r\n    /A/b\r\n\r\nThe above *xpath expression* can also be given the moniker: absolute path\r\n\r\nThe following xpath expressions will also result in the node b:\r\n\r\n    //b, /A//b, and I'm sure a few others\r\n    |`     |\r\n    |      '-  Select all explicitly starting from the root\r\n    '-  Select all implicitly starting from the root\r\n\r\nNow what if I asked you to form the expression leading to *only* the\r\nnode c on the first level after the root?\r\n\r\n    //c or /A//c\r\n\r\nCan't work because these expressions land us all nodes c\r\n\r\n    /A/c\r\n\r\nThat will do the trick. It works because what it is in fact saying is\r\nto *select all c nodes in the level right after node A*. Now what\r\nabout one or more nodes only in the last level?\r\n\r\n    /A//c\r\n\r\nWon't work because again, we get all nodes c on all levels, starting from node A. Now here's part the\r\nbeauty with querying languages like XPath.\r\n\r\nWe can fine tune our selection by specifying the **parent** node(s) the\r\ndesired nodes lie. Now that we're using parent-child terminology,\r\nwe can refer to our desired nodes as the children.\r\n\r\nBut children of what? In case you've forgotten what the graph looks like,\r\nhere it is again:\r\n\r\n\r\n    Graph C\r\n                           A•\r\n                          __|__\r\n                         |     |\r\n                        b•     •c\r\n                        _|_\r\n                       |   |\r\n                      c•   •c\r\n\r\n\r\nClearly, the node named b is the parent of the desired nodes named c.\r\n\r\nThis selects all nodes c of node b of node A\r\n\r\n    /A/b/c\r\n\r\nThis selects the first node c of node b of node A\r\n\r\n    /A/b/c[1]\r\n\r\n\r\nThat should be enough to move forward, or rather backwards to the article.\r\n\r\n* There's was/is a debate on whether the process that eatiht goes through to produce an output can be considered an\r\n_algorithm_ or not. [Read about it here](http://www.reddit.com/r/Python/comments/2pqx2d/just_made_what_i_consider_my_first_algorithm_and/cmzfojb).\r\nWhat would you describe eatiht as? A neat package? A neat algorithm? Neither? I'm interested in hearing you out\r\nat rodrigopala@gmail.com, [twitter](https://twitter.com/rodricios), or [github](https://github.com/rodricios).\r\n\r\n\r\nUpdates:\r\n========\r\n\r\n12/23/2014\r\n----------\r\n* [Prof. Tim Weninger](http://www3.nd.edu/~tweninge/) has informed me that this algorithm is in fact an unsupervised classification algorithm - a type of algorithm that attempts to solve problems without \"training data.\" It's kind of awkward how when I add one more label to this Python Package, it immediately requires some extra explanation of what I just said. Don't worry if \"unsupervised learning\" or \"classification\" or \"training data\" makes no sense; it's unnecessary nomenclature if your learning to program; I'd argue that if you know how to pick out what element appears the most in some set, that's as much as \"machine learning\" - statistics - you need to know. \r\n\r\n* Tim Weninger - a professor at U. of Notre Dame, original author of the Text-to-Tag Clustering paper I referenced in the summary section at the top - was kind enough to reach out on reddit, liked my work, and we're now having closed door meetings; Tim, if you're reading this, please let's meet somewhere in Mid-Western US, seeing as you are from the East, and I'm of the West. \r\n","google":"UA-57938819-1","note":"Don't delete this file! It's used internally to help with page regeneration."}